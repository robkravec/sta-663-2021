{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Programming\n",
    "\n",
    "This is an elementary treatment that covers simple use cases that often arise in statistics and data science, and is focused mostly on parallelism on a multi-core computer. More specialized frameworks for parallel programming such as those listed are not covered:\n",
    "\n",
    "- Message Passing Interface (MPI). Framework for parallelism based on message passing on a compute cluster. Often used when parallel processes need to frequently communicate with each other. More common in scientific than data analysis applications.\n",
    "- GPU computing (CUDA, OpenCL). Framework for exploiting the massively parallel but basic compute units available in modern GPUs. Ideal when you need a large number of simple arithmetic function calls - for example, in deep learning. The [CuPy](https://cupy.chainer.org) library looks like a promising GPU-accelerated alternative to `numpy` if you have a modern NVidia GPU.\n",
    "- Distributed computing (Spark, Dask). Framework for master-worker parallelism on a cluster of commodity machines. Master builds a graph of task dependencies and schedules to execute tasks in the appropriate order.\n",
    "\n",
    "This first lecture is for mainly illustrating concepts. The second will provide more examples of using parallelism in Python programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking machine capabilities on a Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sysctl hw.physicalcpu hw.logicalcpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Unix system, use this instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! lscpu --all --extended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referneces**\n",
    "\n",
    "- [`threading` — Thread-based parallelism](https://docs.python.org/3.8/library/threading.html)\n",
    "- [`multiprocessing` - Process-based “threading” interface](https://docs.python.org/3.8/library/multiprocessing.html)\n",
    "- [`multiprocess` - a \"better\" `multiprocessing`](https://github.com/uqfoundation/multiprocess)\n",
    "- [concurrent.futures — Launching parallel tasks](https://docs.python.org/3/library/concurrent.futures.html#module-concurrent.futures)\n",
    "- [Concurrency with Processes, Threads, and Coroutines](https://pymotw.com/3//concurrency.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common sense\n",
    "\n",
    "- Make it right before trying to make it fast\n",
    "- Weight the trade-off between programmer and program time\n",
    "- Don't blindly optimize - profile code before optimization\n",
    "- A new algorithm or data structure can reduce the complexity class; parallelism generally only offers linear speed ups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Concepts\n",
    "\n",
    "### Concurrent, parallel, distributed\n",
    "\n",
    "- Concurrent programs have tasks with overlapping lifetimes, and concurrent programming is based on units of execution that can be run in any order with the same final result. Concurrent programs can but need not be executed in parallel - for example, the time-slicing performed by the Python's Global Interpreter Lock (GIL), which only allows one thread to be physically executed at any point in time, is an example of concurrency.\n",
    "- Parallel programs run at the same time, for example, on multiple cores. If there is no need to control access to shared resources, these are known as *embarrassingly parallel* programs. All parallel programs are thus concurrent, but not vice versa.\n",
    "- Distributed programs run over multiple computers. The term is most often associated with \"Big Data\" problems, where data transfer is the computing bottleneck - hence the philosophy of \"bring the computation to the data\".\n",
    "\n",
    "### Synchronous and asynchronous calls\n",
    "\n",
    "When there is concurrent access to a shared resource, errors can arise. For example, consider the following code executed by two concurrent processes which share the variable `i` with an initial value of 0:\n",
    "\n",
    "```python\n",
    " i = i + 1\n",
    "```\n",
    "\n",
    "The following may happen: Process 1 reads (`i = 0`), Process 2 reads (`i = 0)`, Process 1 updates (`i = 1`), Process 2 overwrites the value put in by Process 1 (`i = 1`). Hence, after two updates, the value of `i` is 1 and not 2.\n",
    "\n",
    "To avoid this problem, the first process to access the resource may *lock* it for exclusive access:\n",
    "\n",
    "Process 1 reads and locks (`i = 0`), Process 2 tries to access `i` but it is locked, Process 1 updates and releases lock (`i = 1`), Process 2 now reads and locks (`i = 1`), and updates and unlocks (`i = 2`). Hence, after two updates, the value of `i` is 2 as it should be. Note that because the processes are *synchronous*, each process *blocks* access to the shared resource, forcing other process to wait - in Python, this waiting manifests as lack of access to the interpreter until the computation is complete.  In this context, *synchronous* and *blocking* have the same meaning.\n",
    "\n",
    "*Asynchronous* or *non-blocking* calls were designed to allow access to the interpreter at all times - an asynchronous function call returns control to the interpreter immediately, and returns a *future* object that can be used to check at a later point in time if the computation is completed and if so, to return the answer.\n",
    "\n",
    "### Concurrency is difficult\n",
    "\n",
    "Designing programs that share resources or have other dependencies to run concurrently in an effective way is challenging. One of the main reasons for the increasing popularity of functional programming is that concurrency is generally easier with a functional programming style - for example, the famous map-reduce framework for distributed computing uses the `map` and `reduce` abstractions of functional programming.\n",
    "\n",
    "Here we illustrate two famous problems that may arise in concurrent programs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Race condition \n",
    "\n",
    "A *race condition* is one where the result depends on the order of accessing a shared resource, as in the simple example of updating `i` given above.\n",
    "\n",
    "In the example below, up to 4 processes may be trying to increment and assign a new value to val at the same time. Because this takes two steps (increment the RHS, assign to LHS), it can happen that two or more processes increment at the same time, but this is only assigned and counted once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the code works will become clear in the next lecture, but the basic idea is that we have 4 processes trying to increment a value, and because of race conditions, this gives the wrong final count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_with_value(i):\n",
    "    val.value += 1\n",
    "    \n",
    "val = Value('i', 0)\n",
    "with Pool(processes=4) as pool:\n",
    "    pool.map(count_with_value, range(1000))\n",
    "\n",
    "val.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preventing race conditions by locking resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock = Lock()\n",
    "\n",
    "def count_with_lock(i, lock=lock):\n",
    "    lock.acquire()\n",
    "    val.value += 1\n",
    "    lock.release()\n",
    "\n",
    "val = Value('i', 0)\n",
    "with Pool(processes=4) as pool:\n",
    "    pool.map(count_with_lock, range(1000))\n",
    "\n",
    "val.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preventing race conditions by duplicating resources\n",
    "\n",
    "It is usually easier and faster to make copies of resources for each process so that no sharing is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Array\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_with_array(i):\n",
    "    ix = mp.current_process().ident % 4\n",
    "    arr[ix] += 1\n",
    "    \n",
    "arr = Array('i', [0]*4)\n",
    "\n",
    "with Pool(processes=4) as pool:\n",
    "    pool.map(count_with_array, range(1000))\n",
    "\n",
    "arr[:], sum(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deadlock\n",
    "\n",
    "Suppose each process needs both resources `i` and `j` to compute and return a value. Process 1 reads and locks `i` but before it can read `j`, Process 2 reads and locks `j`. Now both processes are waiting but cannot access the other shared resource, and hence the program hangs forever.\n",
    "\n",
    "There is a story about the dining philosophers to help you remember this - a group of philosophers are seated on a round table - on the left of each philosopher is a fork and on the right is a knife. Philosophers need a fork and a knife to eat. At the beginning, each philosopher grabs the utensil to the right - now none of the philosophers have both a knife and a fork, and they starve to death.\n",
    "\n",
    "To break deadlock, a possible strategy is to release locked resources upon failure and wait a random amount of time before trying to acquire it again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amdahl's and Gustafson's laws\n",
    "\n",
    "Suppose you had a million core machine. How much can you speed up your program by parallelizing it?\n",
    "\n",
    "Ahmdahl recognized that the degree of speed-up is driven by the ratio of serial to parallelziable code segments. Serial parts are those where the order is essential $a \\to b \\to c$. Suppose the serial section of code takes up 10% of the the total run-time. Then the maximum speed up 10-fold - the serial part takes up 10% of the time, and you throw one million minus one cores at the parallelizable parts driving their execution time towards zero, giving the 10-fold speed up.\n",
    "\n",
    "Gustafson pointed out that as the size of the problem increases, the time spent in  serial code segments typically goes down. For example, MCMC is inherently *all* serial, but for very large data sets, within each MCMC step, there is typically a need to calculate the likelihood of each data point, and this can be parallelized. We used this idea to achieve over two orders of magnitude speed up for fitting statistical mixture models using GPU computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opportunities for parallelism\n",
    "\n",
    "There are 3 common areas where parallelism can be useful in an algorithm:\n",
    "\n",
    "#### Data\n",
    "\n",
    "- Decomposition of arrays along rows, columns, blocks\n",
    "- Decomposition of trees into sub-trees\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "- Loops\n",
    "- Function calls\n",
    "\n",
    "#### Pipelines\n",
    "\n",
    "If there are a sequence of dependent stages, as soon as one stage completes and hands off to the next stage, it is ready to accept a job from the previous stage\n",
    "\n",
    "- Data processing pipeline\n",
    "\n",
    "$$\n",
    "a \\\\\n",
    "a \\to b \\\\\n",
    "a \\to b \\to c \\\\\n",
    "a \\to b \\to c \\\\\n",
    "a \\to b \\to c \\\\\n",
    "b \\to c \\\\\n",
    "c\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common parallelization idioms\n",
    "\n",
    "- Loop parallelism automatically performs the tasks in a loop in parallel\n",
    "- Fork-join repeatedly splits execution into multiple branches (fork), then merging the branches (join), and is often used for recursive problems.\n",
    "- Single program multiple data (SPMD), where the same program runs on multiple cores on one or more computers, but process different inputs.\n",
    "- Master worker splits processes into a master that generates subproblems for multiple workers to complete "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop parallelism with `joblib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(n):\n",
    "    sleep(n)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "[func(1) for i in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `joblib` library has a list comprehension idiom for loop parallelism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "Parallel(n_jobs=4)(delayed(func)(1) for i in range(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `delayed` function is just a technical device to allow function calls with the usual function call syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_args(n, *args, **kwargs):\n",
    "    sleep(n)\n",
    "    return args, kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "Parallel(n_jobs=4)(delayed(func_args)(1, 2, 3, a=4, b=5) for i in range(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next lecture we will see the use of parallel ranges for loop parallelism with `nuumba` and `cython`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threads and processes\n",
    "\n",
    "We can think of threads and processes as independent workers. The main difference is that threads run in the same memory space, while each process has its own memory space. This means that threads are \"light-weight\" compared to processes - they are faster to create and consume fewer resources.\n",
    "\n",
    "However, because threads run in the same memory space, concurrency issues such as race conditions and deadlock can occur. Because of this, the most commonly used Python interpreter (CPython) uses the [Global Interpreter Lock (GIL)](http://www.dabeaz.com/python/UnderstandingGIL.pdf) to ensure that only a *single* thread is operating in a Python program any one time, and switches between multiple threads to give the illusion of parallelism. The take-home message is that you can use threads for parallel tasks that are are slow because of latency (e.g. network request), but should use processes for parallel tasks that are compute-intensive (e.g. mathematical calculations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_with_latency(n):\n",
    "    sleep(n)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threads work well for high latency processes - e.g. disk access, network requests, await user input etc - here simulated with `sleep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=4) as pool:\n",
    "    pool.map(func_with_latency, [1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as pool:\n",
    "    pool.map(func_with_latency, [1,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not so well for compute-intensive processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_compute_intesnive(n):\n",
    "    s = 0\n",
    "    for i in range(1, n+1):\n",
    "        s += math.exp(math.log(math.sqrt(math.pow(i, 2.0))))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "func_compute_intesnive(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=4) as pool:\n",
    "    pool.map(func_compute_intesnive, [n,n,n,n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as pool:\n",
    "    pool.map(func_compute_intesnive, [n,n,n,n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embarrassingly parallel programs\n",
    "\n",
    "Many statistical problems can be easily decomposed into independent tasks or data sets. Here are several examples:\n",
    "\n",
    "- Monte Carlo integration\n",
    "- Multiple chains of MCMC\n",
    "- Bootstrap for confidence intervals\n",
    "- Power calculations by simulation\n",
    "- Permutation-resampling tests \n",
    "- Fitting same model on multiple data sets\n",
    "\n",
    "These \"low hanging fruits\" are great because they offer a path to easy parallelism with minimal complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool, Value, Array\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Python\n",
    "\n",
    "Toy problem: Estimate $\\pi$ by sampling points at random within a box circumscribing the unit circle and counting the fraction that fall within the circle. This is a simple example of a Monte Carlo algorithm. We will parallelize this embarrassingly parallel problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_pi(n):\n",
    "    s = 0\n",
    "    for i in range(n):\n",
    "        x = np.random.uniform(-1, 1)\n",
    "        y = np.random.uniform(-1, 1)\n",
    "        if (x**2 + y**2) < 1:\n",
    "            s += 1\n",
    "    return 4*s/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mc_pi(int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "res = [mc_pi(int(1e5)) for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `concurrent.futures` module\n",
    "\n",
    "Concurrent processes are processes that will return the same results regardless of the order in which they were executed. A \"future\" is something that will return a result sometime in the future.  The `concurrent.futures` module provides an event handler, which can be fed functions to be scheduled for future execution. This provides us with a simple model for parallel execution on a multi-core machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using processes in parallel with `ProcessPoolExecutor`\n",
    "\n",
    "We get a linear speedup as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=4) as pool:\n",
    "    res = pool.map(mc_pi, [int(1e5) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When you have many jobs\n",
    "\n",
    "The `futures` object gives fine control over the process, such as adding callbacks and canceling a submitted job, but is computationally expensive. We can use the `chunksize` argument to reduce this cost when submitting many jobs - this specifies the number of tasks to be given to a worker at a time. A detailed explanation of `chunksize` is provided [here](https://stackoverflow.com/questions/53751050/python-multiprocessing-understanding-logic-behind-chunksize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using default `chunksize `\n",
    "\n",
    "The total amount of computation whether you have 10 jobs of size 10,000,000 or 10,000 jobs of size 10,000 is essentially the same, so we would expect them both to take about the same amount of time, but this is not true due to the overhead described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=4) as pool:\n",
    "    res = pool.map(mc_pi, [int(1e2) for i in range(int(1e4))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using `chunksize` of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=4) as pool:\n",
    "    res = pool.map(mc_pi, [int(1e2) for i in range(int(1e4))], chunksize=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine control of processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Status of processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return x**2\n",
    "\n",
    "def f2(x, y):\n",
    "    return x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProcessPoolExecutor(max_workers=4) as pool:\n",
    "    a = pool.submit(f2, 1, 1)\n",
    "    b = pool.submit(f2, 1,2)\n",
    "    c = pool.submit(f1, 10)    \n",
    "\n",
    "    print('a running:', a.running())\n",
    "    print('a done:', a.done())\n",
    "\n",
    "    print('b running:', b.running())\n",
    "    print('b done:', b.done())\n",
    "\n",
    "    print('c running:', c.running())\n",
    "    print('c done:', c.done())\n",
    "\n",
    "    print('a result', a.result())\n",
    "    print('b result', b.result())\n",
    "    print('c result', c.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Canceling jobs and adding callbacks\n",
    "\n",
    "For example, if you launch multiple versions of the same task for safety, you might want to cancel the duplicate tasks once one of them has completed.\n",
    "\n",
    "Callbacks are to allow the function to notify you when some event occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "njobs = 24\n",
    "\n",
    "res = []\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=4) as pool:\n",
    "\n",
    "    for i in range(njobs):\n",
    "        res.append(pool.submit(f2, *np.random.rand(2)))\n",
    "        if i % 2 == 0:\n",
    "            res[i].add_done_callback(lambda future: print(\"Process done!\"))\n",
    "    res[4].cancel()\n",
    "    if res[4].cancelled():\n",
    "        print(\"Process 4 cancelled\")\n",
    "\n",
    "    for i, x in enumerate(res):\n",
    "        while x.running():\n",
    "            print(\"Running\")\n",
    "            time.sleep(1)\n",
    "        if not x.cancelled():\n",
    "            print(x.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions with multiple arguments\n",
    "\n",
    "Using a pool and the `map` method with functions requiring multiple arguments can be done in two ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Using a function adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_(args):\n",
    "    return f(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(24)\n",
    "chunks = np.array_split(xs, xs.shape[0]//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProcessPoolExecutor(max_workers=4) as pool:\n",
    "    res = pool.map(f_, chunks)\n",
    "list(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using multiple argument iterables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProcessPoolExecutor(max_workers=4) as pool:\n",
    "    res = pool.map(f, range(0,24,2), range(1,24,2))\n",
    "list(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using processes in parallel with ThreadPoolExecutor\n",
    "\n",
    "We do not get any speedup because the GIL only allows one thread to run at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as pool:\n",
    "    res = pool.map(mc_pi, [int(1e5) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `multiprocessing`\n",
    "\n",
    "The `concurrent.futures.ProcessPoolExecutor` is actually a wrapper for `multiprocessing.Pool` to unify the threading and process interfaces. I typically just work directly with `mutliprocessing` since I don't have much use for threads. One nice thing about using `multiprocessing` apart from more fine-grai control if you need it, is that it typically works equally well for small numbers of large jobs, or large numbers of small jobs out of the box using a heuristic to guess the optimal `chunksize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with mp.Pool(processes=4) as pool:\n",
    "    res = pool.map(mc_pi, [int(1e5) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with mp.Pool(processes=4) as pool:\n",
    "    res = pool.map(mc_pi, [int(1e2) for i in range(int(1e4))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions with multiple arguments\n",
    "\n",
    "Multiprocessing `Pool` has a `starmap` method that removes the need to write a wrapper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(24)\n",
    "with Pool(processes=4) as pool:\n",
    "    res = pool.starmap(f, np.array_split(xs, xs.shape[0]//2))\n",
    "list(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Partial application\n",
    "\n",
    "Sometimes, `functools.partial` can be used to reduce the number of arguments needed to just one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a, b):\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "fp = partial(f, b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(24)\n",
    "with Pool(processes=4) as pool:\n",
    "    res = pool.map(fp, xs)\n",
    "np.array(list(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blocking and non-blocking calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(n):\n",
    "    time.sleep(n)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(processes=4) as pool:\n",
    "    res = pool.map(func, [3,3,3,3,3])\n",
    "    print(\"Control back!\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(processes=4) as pool:\n",
    "    res = pool.map_async(func, [3,3,3,3,3])\n",
    "    print(\"Control back!\")\n",
    "    print(res.ready())\n",
    "    res.wait()\n",
    "    print(res.ready())\n",
    "    print(res.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different jobs to different processes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(n):\n",
    "    time.sleep(1)\n",
    "    return n\n",
    "\n",
    "def f2(n):\n",
    "    time.sleep(1)\n",
    "    return n**2\n",
    "\n",
    "def f3(n):\n",
    "    time.sleep(1)\n",
    "    return n**3\n",
    "\n",
    "def f4(n):\n",
    "    time.sleep(1)\n",
    "    return n**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with Pool(processes=4) as pool:\n",
    "    res = []\n",
    "    for i, f in enumerate([f4, f2, f3, f1]):\n",
    "        res.append((i, pool.apply(f, [2])))\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with Pool(processes=4) as pool:\n",
    "    res = []\n",
    "    for i, f in enumerate([f4, f2, f3, f1]):\n",
    "        res.append((i, pool.apply_async(f, [2])))\n",
    "    print([(i, r.get()) for i, r in res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating individual processes\n",
    "\n",
    "If you need more control over individual processes than Pool provides - namely, if you need to share information across processes, you can work with individual workers and thread-safe memory structures. This is just for completeness as most data processing tasks do not require this level of control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(i):\n",
    "    time.sleep(np.random.random())\n",
    "    print(os.getpid(), i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    p = mp.Process(target=f, args=(i,))\n",
    "    p.start()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Queues to share information between processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(q, i):\n",
    "    time.sleep(np.random.random())\n",
    "    q.put((os.getpid(), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = mp.Queue()\n",
    "\n",
    "res = []\n",
    "for i in range(10):\n",
    "    p = mp.Process(target=f1, args=(q,i,))\n",
    "    p.start()\n",
    "    res.append(q.get())\n",
    "    p.join()\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Value and Array for sharing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting number of jobs (1)\n",
    "\n",
    "This does not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(i):\n",
    "    global counter\n",
    "    counter = counter + 1\n",
    "    print(os.getpid(), i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "f2(10)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for i in range(10):\n",
    "    p = mp.Process(target=f2, args=(i,))\n",
    "    p.start()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note that separate processes have their own memory and DO NOT share global memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting number of jobs (2)\n",
    "\n",
    "We can use shared memory to do this, but it is slow because multiprocessing has to ensure that only one process gets to use counter at any one time. Multiprocesing provides Value and Array shared memory variables, but you can also convert arbitrary Python variables into shared memory objects (less efficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(i, counter, store):\n",
    "    counter.value += 1\n",
    "    store[os.getpid() % 10] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "counter = mp.Value('i', 0)\n",
    "store = mp.Array('i', [0]*10)\n",
    "\n",
    "for i in range(int(1e2)):\n",
    "    p = mp.Process(target=f3, args=(i, counter, store))\n",
    "    p.start()\n",
    "    p.join()\n",
    "\n",
    "print(counter.value)\n",
    "print(store[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avoiding use of shared memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Counting number of jobs (3)\n",
    "\n",
    "We should try to avoid using shared memory as much as possible in parallel jobs as they drastically reduce efficiency. One useful approach is to use the `map-reduce` pattern. We should also use Pool to reuse processes rather than spawn too many of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f4(i):\n",
    "    return (os.getpid(), 1, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# map step\n",
    "with mp.Pool(processes=4) as pool:\n",
    "    res = pool.map(f4, range(int(1e2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reduce steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[np.random.choice(len(res), 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res, columns=['pid', 'one', 'i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('pid').sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
