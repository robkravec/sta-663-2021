{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Roots of Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculus review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as scipy\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's review the theory of optimization for multivariate functions. Recall that in the single-variable case, extreme values (local extrema) occur at points where the first derivative is zero, however, the vanishing of the first derivative is not a sufficient condition for a local max or min.  Generally, we apply the second derivative test to determine whether a candidate point is a max or min (sometimes it fails - if the second derivative either does not exist or is zero).  In the multivariate case, the first and second derivatives are *matrices*.  In the case of a scalar-valued function on $\\mathbb{R}^n$, the first derivative is an $n\\times 1$ vector called the *gradient* (denoted $\\nabla f$). The second derivative is an $n\\times n$ matrix called the *Hessian* (denoted $H$)\n",
    "\n",
    "Just to remind you, the gradient and Hessian are given by:\n",
    "\n",
    "$$\\nabla f(x) = \\left(\\begin{matrix}\\frac{\\partial f}{\\partial x_1}\\\\ \\vdots \\\\\\frac{\\partial f}{\\partial x_n}\\end{matrix}\\right)$$\n",
    "\n",
    "\n",
    "$$H = \\left(\\begin{matrix}\n",
    "  \\dfrac{\\partial^2 f}{\\partial x_1^2} & \\dfrac{\\partial^2 f}{\\partial x_1\\,\\partial x_2} & \\cdots & \\dfrac{\\partial^2 f}{\\partial x_1\\,\\partial x_n} \\\\[2.2ex]\n",
    "  \\dfrac{\\partial^2 f}{\\partial x_2\\,\\partial x_1} & \\dfrac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\dfrac{\\partial^2 f}{\\partial x_2\\,\\partial x_n} \\\\[2.2ex]\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\[2.2ex]\n",
    "  \\dfrac{\\partial^2 f}{\\partial x_n\\,\\partial x_1} & \\dfrac{\\partial^2 f}{\\partial x_n\\,\\partial x_2} & \\cdots & \\dfrac{\\partial^2 f}{\\partial x_n^2}\n",
    "\\end{matrix}\\right)$$\n",
    "\n",
    "One of the first things to note about the Hessian - it's symmetric. This structure leads to some useful properties in terms of interpreting critical points.\n",
    "\n",
    "The multivariate analog of the test for a local max or min turns out to be a statement about the gradient and the Hessian matrix.  Specifically, a function $f:\\mathbb{R}^n\\rightarrow \\mathbb{R}$ has a critical point at $x$ if $\\nabla f(x) = 0$ (where zero is the zero vector!).  Furthermore, the second derivative test at a critical point is as follows:\n",
    "\n",
    "* If $H(x)$ is positive-definite ($\\iff$ it has all positive eigenvalues), $f$ has a local minimum at $x$\n",
    "* If $H(x)$ is negative-definite ($\\iff$ it has all negative eigenvalues), $f$ has a local maximum at $x$\n",
    "* If $H(x)$ has both positive and negative eigenvalues, $f$ has a saddle point at $x$.\n",
    "\n",
    "If you have $m$ equations with $n$ variables, then the $m \\times n$ matrix of first partial derivatives is known as the Jacobian $J(x)$. For example, for two equations $f(x, y)$ and $g(x, y)$, we have\n",
    "\n",
    "$$\n",
    "J(x) = \\begin{bmatrix}\n",
    "\\frac{\\delta f}{\\delta x} & \\frac{\\delta f}{\\delta y} \\\\\n",
    "\\frac{\\delta g}{\\delta x} & \\frac{\\delta g}{\\delta y} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can now express the multivariate form of Taylor polynomials in a familiar format.\n",
    "\n",
    "$$\n",
    "f(x + \\delta x) = f(x) + \\delta x \\cdot J(x) + \\frac{1}{2} \\delta x^T H(x) \\delta x + \\mathcal{O}(\\delta x^3)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Issues in Root Finding in One Dimension\n",
    "\n",
    "* Separating close roots\n",
    "* Numerical Stability\n",
    "* Rate of Convergence\n",
    "* Continuity and Differentiability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bisection Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bisection method is one of the simplest methods for finding zeros of a non-linear function.  It is guaranteed to find a root - but it can be slow. The main idea comes from the intermediate value theorem:  If $f(a)$ and $f(b)$ have different signs and $f$ is continuous, then $f$ must have a zero between $a$ and $b$.  We evaluate the function at the midpoint, $c = \\frac12(a+b)$. $f(c)$ is either zero, has the same sign as $f(a)$ or the same sign as $f(b)$.  Suppose $f(c)$ has the same sign as $f(a)$ (as pictured below).  We then repeat the process on the interval $[c,b]$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**3 + 4*x**2 -3\n",
    "\n",
    "x = np.linspace(-3.1, 0, 100)\n",
    "plt.plot(x, x**3 + 4*x**2 -3)\n",
    "\n",
    "a = -3.0\n",
    "b = -0.5\n",
    "c = 0.5*(a+b)\n",
    "\n",
    "plt.text(a,-1,\"a\")\n",
    "plt.text(b,-1,\"b\")\n",
    "plt.text(c,-1,\"c\")\n",
    "\n",
    "plt.scatter([a,b,c], [f(a), f(b),f(c)], s=50, facecolors='none')\n",
    "plt.scatter([a,b,c], [0,0,0], s=50, c='red')\n",
    "\n",
    "xaxis = plt.axhline(0)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3.1, 0, 100)\n",
    "plt.plot(x, x**3 + 4*x**2 -3)\n",
    "\n",
    "d = 0.5*(b+c)\n",
    "\n",
    "plt.text(d,-1,\"d\")\n",
    "plt.text(b,-1,\"b\")\n",
    "plt.text(c,-1,\"c\")\n",
    "\n",
    "plt.scatter([d,b,c], [f(d), f(b),f(c)], s=50, facecolors='none')\n",
    "plt.scatter([d,b,c], [0,0,0], s=50, c='red')\n",
    "\n",
    "xaxis = plt.axhline(0)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can terminate the process whenever the function evaluated at the new midpoint is 'close enough' to zero. This method is an example of what are known as 'bracketed methods'. This means the root is 'bracketed' by the end-points (it is somewhere in between). Another class of methods are 'open methods' - the root need not be somewhere in between the end-points (but it usually needs to be close!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secant Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The secant method also begins with two initial points, but without the constraint that the function values are of opposite signs.  We use the secant line to extrapolate the next candidate point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x**3-2*x+7)/(x**4+2)\n",
    "\n",
    "x = np.arange(-3,5, 0.1);\n",
    "y = f(x)\n",
    "\n",
    "p1=plt.plot(x, y)\n",
    "plt.xlim(-3, 4)\n",
    "plt.ylim(-.5, 4)\n",
    "plt.xlabel('x')\n",
    "plt.axhline(0)\n",
    "t = np.arange(-10, 5., 0.1)\n",
    "\n",
    "x0=-1.2\n",
    "x1=-0.5\n",
    "xvals = []\n",
    "xvals.append(x0)\n",
    "xvals.append(x1)\n",
    "notconverge = 1\n",
    "count = 0\n",
    "cols=['r--','b--','g--','y--']\n",
    "while (notconverge==1 and count <  3):\n",
    "    slope=(f(xvals[count+1])-f(xvals[count]))/(xvals[count+1]-xvals[count])\n",
    "    intercept=-slope*xvals[count+1]+f(xvals[count+1])\n",
    "    plt.plot(t, slope*t + intercept, cols[count])\n",
    "    nextval = -intercept/slope\n",
    "    if abs(f(nextval)) < 0.001:\n",
    "        notconverge=0\n",
    "    else:\n",
    "        xvals.append(nextval)\n",
    "    count = count+1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The secant method has the advantage of fast convergence.  While the bisection method has a linear convergence rate (i.e. error goes to zero at the rate that $h(x) = x$ goes to zero, the secant method has a convergence rate that is faster than linear, but not quite quadratic (i.e. $\\sim x^\\alpha$, where $\\alpha = \\frac{1+\\sqrt{5}}2 \\approx 1.6$) however, the trade-off is that the secant method is not guaranteed to find a root in the brackets.\n",
    "\n",
    "A variant of the secant method is known as the **method of false positions**. Conceptually it is identical to the secant method, except that instead of always using the last two values of $x$ for linear interpolation, it chooses the two most recent values that maintain the bracket property (i.e $f(a) f(b) < 0$). It is slower than the secant, but like the bisection, is safe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton-Raphson Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the value $\\theta$ so that some (differentiable) function $g(\\theta)=0$. \n",
    "Idea: start with a guess, $\\theta_0$.  Let $\\tilde{\\theta}$ denote the value of $\\theta$ for which $g(\\theta) = 0$ and define $h = \\tilde{\\theta} - \\theta_0$.  Then:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "g(\\tilde{\\theta}) &=& 0 \\\\\\\\\n",
    "&=&g(\\theta_0 + h) \\\\\\\\\n",
    "&\\approx& g(\\theta_0) + hg'(\\theta_0)\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "This implies that \n",
    "\n",
    "$$ h\\approx \\frac{g(\\theta_0)}{g'(\\theta_0)}$$\n",
    "\n",
    "So that\n",
    "\n",
    "$$\\tilde{\\theta}\\approx \\theta_0 - \\frac{g(\\theta_0)}{g'(\\theta_0)}$$\n",
    "\n",
    "Thus, we set our next approximation:\n",
    "\n",
    "$$\\theta_1 = \\theta_0 - \\frac{g(\\theta_0)}{g'(\\theta_0)}$$\n",
    "\n",
    "and we have developed an iterative procedure with:\n",
    "\n",
    "$$\\theta_n = \\theta_{n-1} - \\frac{g(\\theta_{n-1})}{g'(\\theta_{n-1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "Let $$g(x) = \\frac{x^3-2x+7}{x^4+2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-5,5, 0.1);\n",
    "y = (x**3-2*x+7)/(x**4+2)\n",
    "\n",
    "p1=plt.plot(x, y)\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-.5, 4)\n",
    "plt.xlabel('x')\n",
    "plt.axhline(0)\n",
    "plt.title('Example Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-5,5, 0.1);\n",
    "y = (x**3-2*x+7)/(x**4+2)\n",
    "\n",
    "p1=plt.plot(x, y)\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-.5, 4)\n",
    "plt.xlabel('x')\n",
    "plt.axhline(0)\n",
    "plt.title('Good Guess')\n",
    "t = np.arange(-5, 5., 0.1)\n",
    "\n",
    "x0=-1.5\n",
    "xvals = []\n",
    "xvals.append(x0)\n",
    "notconverge = 1\n",
    "count = 0\n",
    "cols=['r--','b--','g--','y--','c--','m--','k--','w--']\n",
    "while (notconverge==1 and count <  6):\n",
    "    funval=(xvals[count]**3-2*xvals[count]+7)/(xvals[count]**4+2)\n",
    "    slope=-((4*xvals[count]**3 *(7 - 2 *xvals[count] + xvals[count]**3))/(2 + xvals[count]**4)**2) + (-2 + 3 *xvals[count]**2)/(2 + xvals[count]**4)\n",
    "   \n",
    "    intercept=-slope*xvals[count]+(xvals[count]**3-2*xvals[count]+7)/(xvals[count]**4+2)\n",
    "\n",
    "    plt.plot(t, slope*t + intercept, cols[count])\n",
    "    nextval = -intercept/slope\n",
    "    if abs(funval) < 0.01:\n",
    "        notconverge=0\n",
    "    else:\n",
    "        xvals.append(nextval)\n",
    "    count = count+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph, we see the zero is near -2.  We make an initial guess of $$x=-1.5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have made an excellent choice for our first guess, and we can see rapid convergence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the Newton-Raphson method converges quadratically.  However, NR (and the secant method) have a fatal flaw:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-5,5, 0.1);\n",
    "y = (x**3-2*x+7)/(x**4+2)\n",
    "\n",
    "p1=plt.plot(x, y)\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-.5, 4)\n",
    "plt.xlabel('x')\n",
    "plt.axhline(0)\n",
    "plt.title('Bad Guess')\n",
    "t = np.arange(-5, 5., 0.1)\n",
    "\n",
    "x0=-0.5\n",
    "xvals = []\n",
    "xvals.append(x0)\n",
    "notconverge = 1\n",
    "count = 0\n",
    "cols=['r--','b--','g--','y--','c--','m--','k--','w--']\n",
    "while (notconverge==1 and count <  6):\n",
    "    funval=(xvals[count]**3-2*xvals[count]+7)/(xvals[count]**4+2)\n",
    "    slope=-((4*xvals[count]**3 *(7 - 2 *xvals[count] + xvals[count]**3))/(2 + xvals[count]**4)**2) + (-2 + 3 *xvals[count]**2)/(2 + xvals[count]**4)\n",
    "   \n",
    "    intercept=-slope*xvals[count]+(xvals[count]**3-2*xvals[count]+7)/(xvals[count]**4+2)\n",
    "\n",
    "    plt.plot(t, slope*t + intercept, cols[count])\n",
    "    nextval = -intercept/slope\n",
    "    if abs(funval) < 0.01:\n",
    "        notconverge = 0\n",
    "    else:\n",
    "        xvals.append(nextval)\n",
    "    count = count+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have stumbled on the horizontal asymptote.  The algorithm fails to converge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a derivation of the convergence rate of the NR method:\n",
    "\n",
    "\n",
    "Suppose $x_k \\; \\rightarrow \\; x^*$ and $g'(x^*) \\neq 0$.  Then we may write:\n",
    "\n",
    "$$x_k = x^* + \\epsilon_k$$.\n",
    "\n",
    "Now expand $g$ at $x^*$:\n",
    "\n",
    "$$g(x_k) = g(x^*) + g'(x^*)\\epsilon_k + \\frac12 g''(x^*)\\epsilon_k^2 + ...$$\n",
    "$$g'(x_k)=g'(x^*) + g''(x^*)\\epsilon_k$$\n",
    "\n",
    "We have that\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\epsilon_{k+1} &=& \\epsilon_k + \\left(x_{k-1}-x_k\\right)\\\\\n",
    "&=& \\epsilon_k -\\frac{g(x_k)}{g'(x_k)}\\\\\n",
    "&\\approx & \\frac{g'(x^*)\\epsilon_k + \\frac12g''(x^*)\\epsilon_k^2}{g'(x^*)+g''(x^*)\\epsilon_k}\\\\\n",
    "&\\approx & \\frac{g''(x^*)}{2g'(x^*)}\\epsilon_k^2\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss-Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 1D, the Newton method is\n",
    "$$\n",
    "x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n",
    "$$\n",
    "\n",
    "We can generalize to $k$ dimensions by \n",
    "$$\n",
    "x_{n+1} = x_n - J^{-1} f(x_n)\n",
    "$$\n",
    "where $x$ and $f(x)$ are now vectors, and $J^{-1}$ is the inverse Jacobian matrix. In general, the Jacobian is not a square matrix, and we use the generalized inverse $(J^TJ)^{-1}J^T$ instead, giving\n",
    "$$\n",
    "x_{n+1} = x_n - (J^TJ)^{-1}J^T f(x_n)\n",
    "$$\n",
    "\n",
    "In multivariate nonlinear estimation problems, we can find the vector of parameters $\\beta$ by minimizing the residuals $r(\\beta)$, \n",
    "$$\n",
    "\\beta_{n+1} = \\beta_n - (J^TJ)^{-1}J^T r(\\beta_n)\n",
    "$$\n",
    "where the entries of the Jacobian matrix $J$ are\n",
    "$$\n",
    "J_{ij} = \\frac{\\partial r_i(\\beta)}{\\partial \\beta_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Quadratic Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse quadratic interpolation is a type of polynomial interpolation.  Polynomial interpolation simply means we find the polynomial of least degree that fits a set of points.  In quadratic interpolation, we use three points, and find the quadratic polynomial that passes through those three points.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f(x):\n",
    "    return (x - 2) * x * (x + 2)**2\n",
    "\n",
    "\n",
    "x = np.arange(-5,5, 0.1);\n",
    "plt.plot(x, f(x))\n",
    "plt.xlim(-3.5, 0.5)\n",
    "plt.ylim(-5, 16)\n",
    "plt.xlabel('x')\n",
    "plt.axhline(0)\n",
    "plt.title(\"Quadratic Interpolation\")\n",
    "\n",
    "#First Interpolation\n",
    "x0=np.array([-3,-2.5,-1.0])\n",
    "y0=f(x0)\n",
    "f2 = interp1d(x0, y0,kind='quadratic')\n",
    "\n",
    "#Plot parabola\n",
    "xs = np.linspace(-3, -1, num=10000, endpoint=True)\n",
    "plt.plot(xs, f2(xs))\n",
    "\n",
    "#Plot first triplet\n",
    "plt.plot(x0, f(x0),'ro');\n",
    "plt.scatter(x0, f(x0), s=50, c='yellow');\n",
    "\n",
    "#New x value\n",
    "xnew=xs[np.where(abs(f2(xs))==min(abs(f2(xs))))]\n",
    "\n",
    "plt.scatter(np.append(xnew,xnew), np.append(0,f(xnew)), c='black');\n",
    "\n",
    "#New triplet\n",
    "x1=np.append([-3,-2.5],xnew)\n",
    "y1=f(x1)\n",
    "f2 = interp1d(x1, y1,kind='quadratic')\n",
    "\n",
    "#New Parabola\n",
    "xs = np.linspace(min(x1), max(x1), num=100, endpoint=True)\n",
    "plt.plot(xs, f2(xs))\n",
    "\n",
    "xnew=xs[np.where(abs(f2(xs))==min(abs(f2(xs))))]\n",
    "plt.scatter(np.append(xnew,xnew), np.append(0,f(xnew)), c='green');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's the idea behind quadratic interpolation. Use a quadratic approximation, find the zero of interest, use that as a new point for the next quadratic approximation.\n",
    "\n",
    "\n",
    "Inverse quadratic interpolation means we do quadratic interpolation on the *inverse function*.  So, if we are looking for a root of $f$, we approximate $f^{-1}(x)$ using quadratic interpolation. This just means fitting $x$ as a function of $y$, so that the quadratic is turned on its side and we are guaranteed that it cuts the x-axis somewhere. Note that the secant method can be viewed as a *linear* interpolation on the inverse of $f$. We can write:\n",
    "\n",
    "$$f^{-1}(y) = \\frac{(y-f(x_n))(y-f(x_{n-1}))}{(f(x_{n-2})-f(x_{n-1}))(f(x_{n-2})-f(x_{n}))}x_{n-2} + \\frac{(y-f(x_n))(y-f(x_{n-2}))}{(f(x_{n-1})-f(x_{n-2}))(f(x_{n-1})-f(x_{n}))}x_{n-1} + \\frac{(y-f(x_{n-2}))(y-f(x_{n-1}))}{(f(x_{n})-f(x_{n-2}))(f(x_{n})-f(x_{n-1}))}x_{n-1}$$\n",
    "\n",
    "We use the above formula to find the next guess $x_{n+1}$ for a zero of $f$ (so $y=0$):\n",
    "\n",
    "$$x_{n+1} = \\frac{f(x_n)f(x_{n-1})}{(f(x_{n-2})-f(x_{n-1}))(f(x_{n-2})-f(x_{n}))}x_{n-2} + \\frac{f(x_n)f(x_{n-2})}{(f(x_{n-1})-f(x_{n-2}))(f(x_{n-1})-f(x_{n}))}x_{n-1} + \\frac{f(x_{n-2})f(x_{n-1})}{(f(x_{n})-f(x_{n-2}))(f(x_{n})-f(x_{n-1}))}x_{n}$$\n",
    "\n",
    "We aren't so much interested in deriving this as we are understanding the procedure:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-5,5, 0.1);\n",
    "plt.plot(x, f(x))\n",
    "plt.xlim(-3.5, 0.5)\n",
    "plt.ylim(-5, 16)\n",
    "plt.xlabel('x')\n",
    "plt.axhline(0)\n",
    "plt.title(\"Inverse Quadratic Interpolation\")\n",
    "\n",
    "#First Interpolation\n",
    "x0=np.array([-3,-2.5,1])\n",
    "y0=f(x0)\n",
    "f2 = interp1d(y0, x0,kind='quadratic')\n",
    "\n",
    "#Plot parabola\n",
    "xs = np.linspace(min(f(x0)), max(f(x0)), num=10000, endpoint=True)\n",
    "plt.plot(f2(xs), xs)\n",
    "\n",
    "#Plot first triplet\n",
    "plt.plot(x0, f(x0),'ro');\n",
    "plt.scatter(x0, f(x0), s=50, c='yellow');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence rate is approximately $1.8$. The advantage of the inverse method is that we will *always* have a real root (the parabola will always cross the x-axis). A serious disadvantage is that the initial points must be very close to the root or the method may not converge.\n",
    "\n",
    "That is why it is usually used in conjunction with other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brentq Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brent's method is a combination of bisection, secant and inverse quadratic interpolation.  Like bisection, it is a 'bracketed' method (starts with points $(a,b)$ such that $f(a)f(b)<0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly speaking, the method begins by using the secant method to obtain a third point $c$, then uses inverse quadratic interpolation to generate the next possible root. Without going into too much detail, the algorithm attempts to assess when interpolation will go awry, and if so, performs a bisection step. Also, it has certain criteria to reject an iterate. If that happens, the next step will be linear interpolation (secant method). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find zeros, use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-5,5, 0.1);\n",
    "p1=plt.plot(x, f(x))\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-10, 20)\n",
    "plt.xlabel('x')\n",
    "plt.axhline(0)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.optimize.brentq(f,-1,.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.optimize.brentq(f,.5,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roots of polynomials\n",
    "\n",
    "One method for finding roots of polynomials converts the problem into an eigenvalue one by using the **companion matrix** of a polynomial. For a polynomial \n",
    "\n",
    "$$\n",
    "p(x) = a_0 + a_1x + a_2 x^2 + \\ldots + a_m x^m\n",
    "$$\n",
    "\n",
    "the companion matrix is\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "-a_{m-1}/a_m & -a_{m-2}/a_m & \\ldots & -a_0/a_m \\\\\n",
    "1 & 0 & \\ldots & 0 \\\\\n",
    "0 & 1 & \\ldots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ldots & \\vdots \\\\\n",
    "0 & 0 & \\ldots & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The characteristic polynomial of the companion matrix is $\\lvert \\lambda I - A \\rvert$ which expands to \n",
    "\n",
    "$$\n",
    "a_0 + a_1 \\lambda + a_2 \\lambda^2 + \\ldots + a_m \\lambda^m\n",
    "$$\n",
    "\n",
    "In other words, the roots we are seeking are the eigenvalues of the companion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to find the cube roots of unity, we solve $x^3 - 1 = 0$. The `roots` function uses the companion matrix method to find roots of polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients of $x^3, x^2, x^1, x^0$\n",
    "\n",
    "poly = np.array([1, 0, 0, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [0,0,1],\n",
    "    [1,0,0],\n",
    "    [0,1,0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.linalg.eigvals(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using built-in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.roots(poly)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter([z.real for z in x], [z.imag for z in x])\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "u = np.cos(theta)\n",
    "v = np.sin(theta)\n",
    "plt.plot(u, v, ':')\n",
    "plt.axis('square')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `scipy.optimize`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding roots of univariate equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**3-3*x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3,3,100)\n",
    "plt.axhline(0, c='red')\n",
    "plt.plot(x, f(x))\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import brentq, newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `brentq` is the recommended method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brentq(f, -3, 0), brentq(f, 0, 1), brentq(f, 1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secant method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton(f, -3), newton(f, 0), newton(f, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Newton-Raphson method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprime = lambda x: 3*x**2 - 3\n",
    "newton(f, -3, fprime), newton(f, 0, fprime), newton(f, 3, fprime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding fixed points\n",
    "\n",
    "Finding the fixed points of a function $g(x) = x$ is the same as finding the roots of $g(x) - x$. However, specialized algorithms also exist - e.g. using `scipy.optimize.fixedpoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fixed_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3,3,100)\n",
    "plt.plot(x, f(x), color='red')\n",
    "plt.plot(x, x)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_point(f, 0), fixed_point(f, -3), fixed_point(f, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutlivariate roots and fixed points\n",
    "\n",
    "Use `root` to solve polynomial equations. Use `fsolve` for non-polynomial equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import root, fsolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to solve a sysetm of $m$ equations with $n$ unknowns\n",
    "\n",
    "\\begin{align}\n",
    "f(x_0, x_1) &= x_1 - 3x_0(x_0+1)(x_0-1) \\\\\n",
    "g(x_0, x_1) &= 0.25 x_0^2 + x_1^2 - 1\n",
    "\\end{align}\n",
    "\n",
    "Note that the equations are non-linear and there can be multiple solutions. These can be interpreted as fixed points of a system of differential equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return [x[1] - 3*x[0]*(x[0]+1)*(x[0]-1),\n",
    "            .25*x[0]**2 + x[1]**2 - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = root(f, (0.5, 0.5))\n",
    "sol.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsolve(f, (0.5, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r0 = root(f,[1,1])\n",
    "r1 = root(f,[0,1])\n",
    "r2 = root(f,[-1,1.1])\n",
    "r3 = root(f,[-1,-1])\n",
    "r4 = root(f,[2,-0.5])\n",
    "\n",
    "roots = np.c_[r0.x, r1.x, r2.x, r3.x, r4.x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, X = np.mgrid[-3:3:100j, -3:3:100j]\n",
    "U = Y - 3*X*(X + 1)*(X-1)\n",
    "V = .25*X**2 + Y**2 - 1\n",
    "\n",
    "plt.streamplot(X, Y, U, V, color=U, linewidth=2, cmap=plt.cm.autumn)\n",
    "plt.scatter(roots[0], roots[1], s=50, c='none', edgecolors='k', linewidth=2)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also give the Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jac(x):\n",
    "    return [[-6*x[0], 1], [0.5*x[0], 2*x[1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = root(f, (0.5, 0.5), jac=jac)\n",
    "sol.x, sol.fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check that values found are really roots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(f(sol.x), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting from other initial conditions, different roots may be found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = root(f, (12,12))\n",
    "sol.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(f(sol.x), 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
