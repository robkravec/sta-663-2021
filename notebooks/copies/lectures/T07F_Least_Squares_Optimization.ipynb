{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least squares optimization\n",
    "\n",
    "Many optimization problems involve minimization of a sum of squared residuals. We will take a look at finding the derivatives for least squares minimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In least squares problems, we usually have $m$ labeled observations $(x_i, y_i)$. We have a model that will predict $y_i$ given $x_i$ for some parameters $\\beta$, $f(x) = X\\beta$. We want to minimize the sum (or average) of squared residuals $r(x_i) = y_i - f(x_i)$. For example, the objective function is usually taken to be \n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\sum{r(x_i)^2}\n",
    "$$\n",
    "\n",
    "As a concrete example, suppose we want to fit a quadratic function to some observed data. We have\n",
    "\n",
    "$$\n",
    "f(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2\n",
    "$$\n",
    "\n",
    "We want to minimize the objective function\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2} \\sum_{i=1}^m (y_i - f(x_i))^2\n",
    "$$\n",
    "\n",
    "Taking derivatives with respect to $\\beta$, we get\n",
    "\n",
    "$$\n",
    "\\frac{dL}{d\\beta} = \n",
    "\\begin{bmatrix}\n",
    "\\sum_{i=1}^m f(x_i) - y_i \\\\\n",
    "\\sum_{i=1}^m x_i (f(x_i) - y_i) \\\\\n",
    "\\sum_{i=1}^m x_i^2 (f(x_i) - y_i)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with matrices\n",
    "\n",
    "Writing the above system as a matrix, we have $f(x) = X\\beta$, with\n",
    "\n",
    "$$ \n",
    "X = \\begin{bmatrix}\n",
    "1 & x_1 & x_1^2 \\\\\n",
    "1 & x_2 & x_2^2 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_m & x_m^2 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\beta = \\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\beta_2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We want to find the derivative of $\\Vert y - X\\beta \\Vert^2$, so\n",
    "\n",
    "$$\n",
    "\\Vert y - X\\beta \\Vert^2 \\\\\n",
    "= (y - X\\beta)^T(y - X\\beta) \\\\\n",
    "= (y^T - \\beta^TX^T)(y - X\\beta) \\\\\n",
    "= y^Ty - \\beta^TX^Ty -y^TX\\beta + \\beta^TX^TX\\beta\n",
    "$$\n",
    "\n",
    "Taking derivatives with respect to $\\beta^T$ (we do this because the gradient is traditionally a row vector, and we want it as a column vector here), we get (after multiplying by $1/2$ for the residue function)\n",
    "\n",
    "$$\n",
    "\\frac{dL}{d\\beta^T} =  X^TX\\beta - X^Ty\n",
    "$$\n",
    "\n",
    "For example, if we are doing gradient descent, the update equation is\n",
    "\n",
    "$$\n",
    "\\beta_{k+1} = \\beta_k + \\alpha (X^TX\\beta - X^Ty)\n",
    "$$\n",
    "\n",
    "Note that if we set the derivative to zero and solve, we get\n",
    "\n",
    "$$\n",
    "X^TX\\beta - X^Ty = 0\n",
    "$$\n",
    "\n",
    "and the normal equations\n",
    "\n",
    "$$\n",
    "\\beta = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "For large $X$, solving the normal equations can be more expensive than simpler gradient descent. Note that the Levenberg-Marquadt algorithm is often used to optimize least squares problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "You are given the following set of data to fit a quadratic polynomial to:\n",
    "\n",
    "```python\n",
    "x = np.arange(10)\n",
    "y = np.array([  1.58873597,   7.55101533,  10.71372171,   7.90123225,\n",
    "                -2.05877605, -12.40257359, -28.64568712, -46.39822281,\n",
    "                -68.15488905, -97.16032044])\n",
    "```\n",
    "\n",
    "Find the least squares solution using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "y = np.array([  1.58873597,   7.55101533,  10.71372171,   7.90123225,\n",
    "                -2.05877605, -12.40257359, -28.64568712, -46.39822281,\n",
    "                -68.15488905, -97.16032044])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y, b):\n",
    "    return (b[0] + b[1]*x + b[2]*x**2 - y)\n",
    "\n",
    "def res(x, y, b):\n",
    "    return sum(f(x,y, b)*f(x, y, b))\n",
    "\n",
    "# Elementary form of gradient\n",
    "def grad(x, y, b):\n",
    "    n = len(x)\n",
    "    return np.array([\n",
    "            sum(f(x, y, b)),\n",
    "            sum(x*f(x, y, b)),\n",
    "            sum(x**2*f(x, y, b))\n",
    "    ])\n",
    "\n",
    "# Matrix form of gradient\n",
    "def grad_m(X, y, b):\n",
    "    return X.T@X@b- X.T@y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad(x, y, np.zeros(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones(len(x)), x, x**2]\n",
    "grad_m(X, y, np.zeros(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import solve\n",
    "\n",
    "beta1 = solve(X.T@X, X.T@y)\n",
    "beta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.0001 # learning rate\n",
    "beta2 = np.zeros(3)\n",
    "for i in range(max_iter):\n",
    "    beta2 -= a * grad(x, y, beta2)\n",
    "beta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.0001 # learning rate\n",
    "beta3 = np.zeros(3)\n",
    "for i in range(max_iter):\n",
    "    beta3 -= a * grad_m(X, y, beta3)\n",
    "beta3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['svd', 'elementary', 'matrix']\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "for i, beta in enumerate([beta1, beta2, beta3], 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.scatter(x, y, s=30)\n",
    "    plt.plot(x, beta[0] + beta[1]*x + beta[2]*x**2, color='red')\n",
    "    plt.title(titles[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Curve fitting and least squares optimization\n",
    "\n",
    "As shown above, least squares optimization is the technique most associated with curve fitting. For convenience, `scipy.optimize` provides a `curve_fit` function that uses Levenberg-Marquadt for minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic4(x, a, b, c, d):\n",
    "    \"\"\"The four paramter logistic function is often used to fit dose-response relationships.\"\"\"\n",
    "    return ((a-d)/(1.0+((x/c)**b))) + d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobs = 24\n",
    "xdata = np.linspace(0.5, 3.5, nobs)\n",
    "ptrue = [10, 3, 1.5, 12]\n",
    "ydata = logistic4(xdata, *ptrue) + 0.5*np.random.random(nobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popt, pcov = curve_fit(logistic4, xdata, ydata) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perr = yerr=np.sqrt(np.diag(pcov))\n",
    "print('Param\\tTrue\\tEstim (+/- 1 SD)')\n",
    "for p, pt, po, pe  in zip('abcd', ptrue, popt, perr):\n",
    "    print('%s\\t%5.2f\\t%5.2f (+/-%5.2f)' % (p, pt, po, pe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 4, 100)\n",
    "y = logistic4(x, *popt)\n",
    "plt.plot(xdata, ydata, 'o')\n",
    "plt.plot(x, y)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
