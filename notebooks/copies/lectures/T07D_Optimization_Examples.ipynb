{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using optimization routines from `scipy` and `statsmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as la\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `scipy.optimize`\n",
    "----\n",
    "\n",
    "One of the most convenient libraries to use is `scipy.optimize`, since it is already part of the Anaconda installation and it has a fairly intuitive interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimizing a univariate function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**4 + 3*(x-2)**3 - 15*(x)**2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-8, 5, 100)\n",
    "plt.plot(x, f(x));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`minimize_scalar`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize_scalar.html#scipy.optimize.minimize_scalar) function will find the minimum, and can also be told to search within given bounds. By default, it uses the Brent algorithm, which combines a bracketing strategy with a parabolic approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.minimize_scalar(f, method='Brent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.minimize_scalar(f, method='bounded', bounds=[0, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local and global minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, offset):\n",
    "    return -np.sinc(x-offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-20, 20, 100)\n",
    "plt.plot(x, f(x, 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note how additional function arguments are passed in\n",
    "sol = opt.minimize_scalar(f, args=(5,))\n",
    "sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, f(x, 5))\n",
    "plt.axvline(sol.x, c='red')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can try multiple random starts to find the global minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = np.random.uniform(-20, 20, 100)\n",
    "upper = lower + 1\n",
    "sols = [opt.minimize_scalar(f, args=(5,), bracket=(l, u)) for (l, u) in zip(lower, upper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmin([sol.fun for sol in sols])\n",
    "sol = sols[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, f(x, 5))\n",
    "plt.axvline(sol.x, c='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a stochastic algorithm\n",
    "\n",
    "See documentation for the [`basinhopping`](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.basinhopping.html) algorithm, which also works with multivariate scalar optimization. Note that this is heuristic and not guaranteed to find a global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import basinhopping\n",
    "\n",
    "x0 = 0\n",
    "sol = basinhopping(f, x0, stepsize=1, minimizer_kwargs={'args': (5,)})\n",
    "sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, f(x, 5))\n",
    "plt.axvline(sol.x, c='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constrained optimization with `scipy.optimize`\n",
    "\n",
    "Many real-world optimization problems have constraints - for example, a set of parameters may have to sum to 1.0 (equality constraint), or some parameters may have to be non-negative (inequality constraint). Sometimes, the constraints can be incorporated into the function to be minimized, for example, the non-negativity constraint $p \\gt 0$ can be removed by substituting $p = e^q$ and optimizing for $q$. Using such workarounds, it may be possible to convert a constrained optimization problem into an unconstrained one, and use the methods discussed above to solve the problem.\n",
    "\n",
    "Alternatively, we can use optimization methods that allow the specification of constraints directly in the problem statement as shown in this section. Internally, constraint violation penalties, barriers and Lagrange multipliers are some of the methods used used to handle these constraints. We use the example provided in the Scipy [tutorial](http://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html) to illustrate how to set constraints.\n",
    "\n",
    "We will optimize:\n",
    "\n",
    "$$\n",
    "f(x) = -(2xy + 2x - x^2 -2y^2)\n",
    "$$\n",
    "subject to the constraint\n",
    "$$\n",
    "x^3 - y = 0 \\\\\n",
    "y - (x-1)^4 - 2 \\ge 0\n",
    "$$\n",
    "and the bounds\n",
    "$$\n",
    "0.5 \\le x \\le 1.5 \\\\\n",
    "1.5 \\le y \\le 2.5\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return -(2*x[0]*x[1] + 2*x[0] - x[0]**2 - 2*x[1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 3, 100)\n",
    "y = np.linspace(0, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(np.vstack([X.ravel(), Y.ravel()])).reshape((100,100))\n",
    "plt.contour(X, Y, Z, np.arange(-1.99,10, 1), cmap='jet');\n",
    "plt.plot(x, x**3, 'k:', linewidth=1)\n",
    "plt.plot(x, (x-1)**4+2, 'k:', linewidth=1)\n",
    "plt.fill([0.5,0.5,1.5,1.5], [2.5,1.5,1.5,2.5], alpha=0.3)\n",
    "plt.axis([0,3,0,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set constraints, we pass in a dictionary with keys `type`, `fun` and `jac`. Note that the inequality constraint assumes a $C_j x \\ge 0$ form. As usual, the `jac` is optional and will be numerically estimated if not provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = ({'type': 'eq',\n",
    "         'fun' : lambda x: np.array([x[0]**3 - x[1]]),\n",
    "         'jac' : lambda x: np.array([3.0*(x[0]**2.0), -1.0])},\n",
    "        {'type': 'ineq',\n",
    "         'fun' : lambda x: np.array([x[1] - (x[0]-1)**4 - 2])})\n",
    "\n",
    "bnds = ((0.5, 1.5), (1.5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = [0, 2.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unconstrained optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ux = opt.minimize(f, x0, constraints=None)\n",
    "ux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constrained optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cx = opt.minimize(f, x0, bounds=bnds, constraints=cons)\n",
    "cx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 3, 100)\n",
    "y = np.linspace(0, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(np.vstack([X.ravel(), Y.ravel()])).reshape((100,100))\n",
    "plt.contour(X, Y, Z, np.arange(-1.99,10, 1), cmap='jet');\n",
    "plt.plot(x, x**3, 'k:', linewidth=1)\n",
    "plt.plot(x, (x-1)**4+2, 'k:', linewidth=1)\n",
    "plt.text(ux['x'][0], ux['x'][1], 'x', va='center', ha='center', size=20, color='blue')\n",
    "plt.text(cx['x'][0], cx['x'][1], 'x', va='center', ha='center', size=20, color='red')\n",
    "plt.fill([0.5,0.5,1.5,1.5], [2.5,1.5,1.5,2.5], alpha=0.3)\n",
    "plt.axis([0,3,0,3]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some applications of optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding paraemeters for ODE models\n",
    "\n",
    "This is a specialized application of `curve_fit`, in which the curve to be fitted is defined implicitly by an ordinary differential equation \n",
    "$$\n",
    "\\frac{dx}{dt} = -kx\n",
    "$$\n",
    "and we want to use observed data to estimate the parameters $k$ and the initial value $x_0$. Of course this can be explicitly solved but the same approach can be used to find multiple parameters for $n$-dimensional systems of ODEs.\n",
    "\n",
    "[A more elaborate example for fitting a system of ODEs to model the zombie apocalypse](http://adventuresinpython.blogspot.com/2012/08/fitting-differential-equation-system-to.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import odeint\n",
    "\n",
    "def f(x, t, k):\n",
    "    \"\"\"Simple exponential decay.\"\"\"\n",
    "    return -k*x\n",
    "\n",
    "def x(t, k, x0):\n",
    "    \"\"\"\n",
    "    Solution to the ODE x'(t) = f(t,x,k) with initial condition x(0) = x0\n",
    "    \"\"\"\n",
    "    x = odeint(f, x0, t, args=(k,))\n",
    "    return x.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameter values\n",
    "x0_ = 10\n",
    "k_ = 0.1*np.pi\n",
    "\n",
    "# Some random data genererated from closed form solution plus Gaussian noise\n",
    "ts = np.sort(np.random.uniform(0, 10, 200))\n",
    "xs = x0_*np.exp(-k_*ts) + np.random.normal(0,0.1,200)\n",
    "\n",
    "popt, cov = opt.curve_fit(x, ts, xs)\n",
    "k_opt, x0_opt = popt\n",
    "\n",
    "print(\"k = %g\" % k_opt)\n",
    "print(\"x0 = %g\" % x0_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "t = np.linspace(0, 10, 100)\n",
    "plt.plot(ts, xs, 'r.', t, x(t, k_opt, x0_opt), '-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example of fitting a system of ODEs using the `lmfit` package\n",
    "\n",
    "You may have to install the [`lmfit`](http://cars9.uchicago.edu/software/python/lmfit/index.html) package using `pip` and restart your kernel. The `lmfit` algorithm is another wrapper around `scipy.optimize.leastsq` but allows for richer model specification and more diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install lmfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmfit import minimize, Parameters, Parameter, report_fit\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(xs, t, ps):\n",
    "    \"\"\"Lotka-Volterra predator-prey model.\"\"\"\n",
    "    try:\n",
    "        a = ps['a'].value\n",
    "        b = ps['b'].value\n",
    "        c = ps['c'].value\n",
    "        d = ps['d'].value\n",
    "    except:\n",
    "        a, b, c, d = ps\n",
    "        \n",
    "    x, y = xs\n",
    "    return [a*x - b*x*y, c*x*y - d*y]\n",
    "\n",
    "def g(t, x0, ps):\n",
    "    \"\"\"\n",
    "    Solution to the ODE x'(t) = f(t,x,k) with initial condition x(0) = x0\n",
    "    \"\"\"\n",
    "    x = odeint(f, x0, t, args=(ps,))\n",
    "    return x\n",
    "\n",
    "def residual(ps, ts, data):\n",
    "    x0 = ps['x0'].value, ps['y0'].value\n",
    "    model = g(ts, x0, ps)\n",
    "    return (model - data).ravel()\n",
    "\n",
    "t = np.linspace(0, 10, 100)\n",
    "x0 = np.array([1,1])\n",
    "\n",
    "a, b, c, d = 3,1,1,1\n",
    "true_params = np.array((a, b, c, d))\n",
    "\n",
    "np.random.seed(123)\n",
    "data = g(t, x0, true_params)\n",
    "data += np.random.normal(size=data.shape)\n",
    "\n",
    "# set parameters incluing bounds\n",
    "params = Parameters()\n",
    "params.add('x0', value= float(data[0, 0]), min=0, max=10)  \n",
    "params.add('y0', value=float(data[0, 1]), min=0, max=10)  \n",
    "params.add('a', value=2.0, min=0, max=10)\n",
    "params.add('b', value=2.0, min=0, max=10)\n",
    "params.add('c', value=2.0, min=0, max=10)\n",
    "params.add('d', value=2.0, min=0, max=10)\n",
    "\n",
    "# fit model and find predicted values\n",
    "result = minimize(residual, params, args=(t, data), method='leastsq')\n",
    "final = data + result.residual.reshape(data.shape)\n",
    "\n",
    "# plot data and fitted curves\n",
    "plt.plot(t, data, 'o')\n",
    "plt.plot(t, final, '-', linewidth=2);\n",
    "\n",
    "# display fitted statistics\n",
    "report_fit(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization of graph node placement\n",
    "\n",
    "To show the many different applications of optimization, here is an example using optimization to change the layout of nodes of a graph. We use a physical analogy - nodes are connected by springs, and the springs resist deformation from their natural length $l_{ij}$. Some nodes are pinned to their initial locations while others are free to move. Because the initial configuration of nodes does not have springs at their natural length, there is tension resulting in a high potential energy $U$, given by the physics formula shown below. Optimization finds the configuration of lowest potential energy given that some nodes are fixed (set up as boundary constraints on the positions of the nodes).\n",
    "\n",
    "$$\n",
    "U = \\frac{1}{2}\\sum_{i,j=1}^n ka_{ij}\\left(||p_i - p_j||-l_{ij}\\right)^2\n",
    "$$\n",
    "\n",
    "Note that the ordination algorithm Multi-Dimensional Scaling (MDS) works on a very similar idea - take a high dimensional data set in $\\mathbb{R}^n$, and project down to a lower dimension ($\\mathbb{R}^k$) such that the sum of distances $d_n(x_i, x_j) - d_k(x_i, x_j)$, where $d_n$ and $d_k$ are some measure of distance between two points $x_i$ and $x_j$ in $n$ and $d$ dimension respectively, is minimized. MDS is often used in exploratory analysis of high-dimensional data to get some intuitive understanding of its \"structure\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P0 is the initial location of nodes\n",
    "- P is the minimal energy location of nodes given constraints\n",
    "- A is a connectivity matrix - there is a spring between $i$ and $j$ if $A_{ij} = 1$\n",
    "- $L_{ij}$ is the resting length of the spring connecting $i$ and $j$\n",
    "- In addition, there are a number of `fixed` nodes whose positions are pinned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "k = 1 # spring stiffness\n",
    "P0 = np.random.uniform(0, 5, (n,2)) \n",
    "A = np.ones((n, n))\n",
    "A[np.tril_indices_from(A)] = 0\n",
    "L = A.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy(P):\n",
    "    P = P.reshape((-1, 2))\n",
    "    D = squareform(pdist(P))\n",
    "    return 0.5*(k * A * (D - L)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D0 = squareform(pdist(P0))\n",
    "E0 = 0.5* k * A * (D0 - L)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D0[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E0[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy(P0.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the position of the first few nodes just to show constraints\n",
    "fixed = 4\n",
    "bounds = (np.repeat(P0[:fixed,:].ravel(), 2).reshape((-1,2)).tolist() + \n",
    "          [[None, None]] * (2*(n-fixed)))\n",
    "bounds[:fixed*2+4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = opt.minimize(energy, P0.ravel(), bounds=bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization\n",
    "\n",
    "Original placement is BLUE\n",
    "Optimized arrangement is RED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(P0[:, 0], P0[:, 1], s=25)\n",
    "P = sol.x.reshape((-1,2))\n",
    "plt.scatter(P[:, 0], P[:, 1], edgecolors='red', facecolors='none', s=30, linewidth=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization of standard statistical models\n",
    "---\n",
    "\n",
    "When we solve standard statistical problems, an optimization procedure similar to the ones discussed here is performed. For example, consider multivariate logistic regression - typically, a Newton-like algorithm known as iteratively reweighted least squares (IRLS) is used to find the maximum likelihood estimate for the generalized linear model family. However, using one of the multivariate scalar minimization methods shown above will also work, for example, the BFGS minimization algorithm. \n",
    "\n",
    "The take home message is that there is nothing magic going on when Python or R fits a statistical model using a formula - all that is happening is that the objective function is set to be the negative of the log likelihood, and the minimum found using some first or second order optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression as optimization\n",
    "\n",
    "Suppose we have a binary outcome measure $Y \\in {0,1}$ that is conditinal on some input variable (vector) $x \\in (-\\infty, +\\infty)$. Let the conditioanl probability be $p(x) = P(Y=y | X=x)$. Given some data, one simple probability model is $p(x) = \\beta_0 + x\\cdot\\beta$ - i.e. linear regression. This doesn't really work for the obvious reason that $p(x)$ must be between 0 and 1 as $x$ ranges across the real line. One simple way to fix this is to use the transformation $g(x) = \\frac{p(x)}{1 - p(x)} = \\beta_0 + x.\\beta$. Solving for $p$, we get\n",
    "$$\n",
    "p(x) = \\frac{1}{1 + e^{-(\\beta_0 + x\\cdot\\beta)}}\n",
    "$$\n",
    "As you all know very well, this is logistic regression.\n",
    "\n",
    "Suppose we have $n$ data points $(x_i, y_i)$ where $x_i$ is a vector of features and $y_i$ is an observed class (0 or 1). For each event, we either have \"success\" ($y = 1$) or \"failure\" ($Y = 0$), so the likelihood looks like the product of Bernoulli random variables. According to the logistic model, the probability of success is $p(x_i)$ if $y_i = 1$ and $1-p(x_i)$ if $y_i = 0$. So the likelihood is\n",
    "$$\n",
    "L(\\beta_0, \\beta) = \\prod_{i=1}^n p(x_i)^y(1-p(x_i))^{1-y}\n",
    "$$\n",
    "and the log-likelihood is \n",
    "\\begin{align}\n",
    "l(\\beta_0, \\beta) &= \\sum_{i=1}^{n} y_i \\log{p(x_i)} + (1-y_i)\\log{1-p(x_i)} \\\\\n",
    "&= \\sum_{i=1}^{n} \\log{1-p(x_i)} + \\sum_{i=1}^{n} y_i \\log{\\frac{p(x_i)}{1-p(x_i)}} \\\\\n",
    "&= \\sum_{i=1}^{n} -\\log 1 + e^{\\beta_0 + x_i\\cdot\\beta} + \\sum_{i=1}^{n} y_i(\\beta_0 + x_i\\cdot\\beta)\n",
    "\\end{align}\n",
    "\n",
    "Using the standard 'trick', if we augment the matrix $X$ with a column of 1s, we can write $\\beta_0 + x_i\\cdot\\beta$ as just $X\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.read_csv(\"binary.csv\")\n",
    "df_.columns = df_.columns.str.lower()\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will ignore the rank categorical value\n",
    "\n",
    "cols_to_keep = ['admit', 'gre', 'gpa']\n",
    "df = df_[cols_to_keep]\n",
    "df.insert(1, 'dummy', 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving as a GLM with IRLS\n",
    "\n",
    "This is very similar to what you would do in R, only using Python's `statsmodels` package. The GLM solver uses a special variant of Newton's method known as iteratively reweighted least squares (IRLS), which will be further desribed in the lecture on multivarite and constrained optimizaiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.GLM.from_formula('admit ~ gre + gpa', \n",
    "                            data=df, family=sm.families.Binomial())\n",
    "fit = model.fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or use R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i df\n",
    "m <- glm(admit ~ gre + gpa, data=df, family=\"binomial\")\n",
    "summary(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home-brew logistic regression using a generic minimization function\n",
    "\n",
    "This is to show that there is no magic going on - you can write the function to minimize directly from the log-likelihood equation and run a minimizer. It will be more accurate if you also provide the derivative (+/- the Hessian for second order methods), but using just the function and numerical approximations to the derivative will also work. As usual, this is for illustration so you understand what is going on - when there is a library function available, you should probably use that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(beta, y, x):\n",
    "    \"\"\"Minus log likelihood function for logistic regression.\"\"\"\n",
    "    return -((-np.log(1 + np.exp(np.dot(x, beta)))).sum() + (y*(np.dot(x, beta))).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta0 = np.zeros(3)\n",
    "opt.minimize(f, beta0, args=(df['admit'], df.loc[:, 'dummy':]), method='BFGS', options={'gtol':1e-2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization with `sklearn`\n",
    "\n",
    "There are also many optimization routines in the `scikit-learn` package, as you already know from the previous lectures. Many machine learning problems essentially boil down to the minimization of some appropriate loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- [Scipy Optimize reference](http://docs.scipy.org/doc/scipy/reference/optimize.html)\n",
    "- [Scipy Optimize tutorial](http://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html)\n",
    "- [LMFit - a modeling interface for nonlinear least squares problems](http://cars9.uchicago.edu/software/python/lmfit/index.html)\n",
    "- [CVXpy- a modeling interface for convex optimization problems](https://github.com/cvxgrp/cvxpy)\n",
    "- [Quasi-Newton methods](http://en.wikipedia.org/wiki/Quasi-Newton_method)\n",
    "- [Convex optimization book by Boyd & Vandenberghe](http://stanford.edu/~boyd/cvxbook/)\n",
    "- [Nocedal and Wright textbook](http://www.springer.com/us/book/9780387303031)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
