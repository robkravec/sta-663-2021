{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Optimizations\n",
    "\n",
    "Mini-batch and stochastic gradient descent is widely used in deep learning, where the large number of parameters and  limited memory make the use of more sophisticated optimization methods impractical. Many methods have been proposed to accelerate gradient descent in this context, and here we sketch the ideas behind some of the most popular algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing with exponentially weighted averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "x = np.arange(n) * np.pi\n",
    "y = np.cos(x) * np.exp(x/100) - 10*np.exp(-0.01*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponentially weighted average\n",
    "\n",
    "The exponentially weighted average adds a fraction $\\beta$ of the current value to a leaky running sum of past values. Effectively, the contribution from the $t-n$th value is scaled by\n",
    "\n",
    "$$\n",
    "\\beta^n(1 - \\beta)\n",
    "$$\n",
    "\n",
    "For example, here are the contributions to the current value after 5 iterations (iteration 5 is the current iteration)\n",
    "\n",
    "| iteration | contribution |\n",
    "| --- | --- |\n",
    "| 1 | $\\beta^4(1 - \\beta)$ |\n",
    "| 2 | $\\beta^3(1 - \\beta)$ |\n",
    "| 3 | $\\beta^2(1 - \\beta)$ |\n",
    "| 4 | $\\beta^1(1 - \\beta)$ |\n",
    "| 5 | $(1 - \\beta)$ |\n",
    "\n",
    "Since $\\beta \\lt 1$, the contribution decreases exponentially with the passage of time. Effectively, this acts as a smoother for a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewa(y, beta):\n",
    "    \"\"\"Exponentially weighted average.\"\"\"\n",
    "    \n",
    "    n = len(y)\n",
    "    zs = np.zeros(n)\n",
    "    z = 0\n",
    "    for i in range(n):\n",
    "        z = beta*z + (1 - beta)*y[i]\n",
    "        zs[i] = z\n",
    "    return zs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponentially weighted average with bias correction\n",
    "\n",
    "Since the EWA starts from 0, there is an initial bias. This can be corrected by scaling with \n",
    "\n",
    "$$\n",
    "\\frac{1}{1 - \\beta^t}\n",
    "$$\n",
    "\n",
    "where $t$ is the iteration number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewabc(y, beta):\n",
    "    \"\"\"Exponentially weighted average with bias correction.\"\"\"\n",
    "    \n",
    "    n = len(y)\n",
    "    zs = np.zeros(n)\n",
    "    z = 0\n",
    "    for i in range(n):\n",
    "        z = beta*z + (1 - beta)*y[i]\n",
    "        zc = z/(1 - beta**(i+1))\n",
    "        zs[i] = zc\n",
    "    return zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.9\n",
    "\n",
    "plt.plot(x, y, 'o-')\n",
    "plt.plot(x, ewa(y, beta), c='red', label='EWA')\n",
    "plt.plot(x, ewabc(y, beta), c='orange', label='EWA with bias correction')\n",
    "plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum in 1D\n",
    "\n",
    "Momentum comes from physics, where the contribution of the gradient is to the velocity, not the position. Hence we create an accessory variable $v$ and increment it with the gradient. The position is then updated with the velocity in place of the gradient. The analogy is that we can think of the parameter $x$ as a particle in an energy well with potential energy $U = mgh$ where $h$ is given by our objective function $f$. The force generated is a function of the rat of change of potential energy $F \\propto \\nabla U \\propto \\nabla f$, and we use $F = ma$ to get that the acceleration $a \\propto \\nabla f$. Finally, we integrate $a$ over time to get the velocity $v$ and integrate $v$ to get the displacement $x$. Note that we need to damp the velocity otherwise the particle would just oscillate forever.\n",
    "\n",
    "We use a version of the update that simply treats the velocity as an exponentially weighted average popularized by Andrew Ng in his Coursera course. This is the same as the momentum scheme motivated by physics with some rescaling of constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(x):\n",
    "    return 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(x, grad, alpha, max_iter=10):\n",
    "    xs = np.zeros(1 + max_iter)\n",
    "    xs[0] = x\n",
    "    for i in range(max_iter):\n",
    "        x = x - alpha * grad(x)\n",
    "        xs[i+1] = x\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_momentum(x, grad, alpha, beta=0.9, max_iter=10):\n",
    "    xs = np.zeros(1 + max_iter)\n",
    "    xs[0] = x\n",
    "    v = 0\n",
    "    for i in range(max_iter):\n",
    "        v = beta*v + (1-beta)*grad(x)\n",
    "        vc = v/(1+beta**(i+1))\n",
    "        x = x - alpha * vc\n",
    "        xs[i+1] = x\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with moderate step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "x0 = 1\n",
    "xs = gd(x0, grad, alpha)\n",
    "xp = np.linspace(-1.2, 1.2, 100)\n",
    "plt.plot(xp, f(xp))\n",
    "plt.plot(xs, f(xs), 'o-', c='red')\n",
    "for i, (x, y) in enumerate(zip(xs, f(xs)), 1):\n",
    "    plt.text(x, y+0.2, i, \n",
    "             bbox=dict(facecolor='yellow', alpha=0.5), fontsize=14)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with large step size\n",
    "\n",
    "When the step size is too large, gradient descent can oscillate and even diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.95\n",
    "xs = gd(1, grad, alpha)\n",
    "xp = np.linspace(-1.2, 1.2, 100)\n",
    "plt.plot(xp, f(xp))\n",
    "plt.plot(xs, f(xs), 'o-', c='red')\n",
    "for i, (x, y) in enumerate(zip(xs, f(xs)), 1):\n",
    "    plt.text(x*1.2, y, i,\n",
    "             bbox=dict(facecolor='yellow', alpha=0.5), fontsize=14)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with momentum\n",
    "\n",
    "Momentum results in cancellation of gradient changes in opposite directions, and hence damps out oscillations while amplifying consistent changes in the same direction. This is perhaps clearer in the 2D example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.95\n",
    "xs = gd_momentum(1, grad, alpha, beta=0.9)\n",
    "xp = np.linspace(-1.2, 1.2, 100)\n",
    "plt.plot(xp, f(xp))\n",
    "plt.plot(xs, f(xs), 'o-', c='red')\n",
    "for i, (x, y) in enumerate(zip(xs, f(xs)), 1):\n",
    "    plt.text(x, y+0.2, i, \n",
    "             bbox=dict(facecolor='yellow', alpha=0.5), fontsize=14)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum and RMSprop in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x):\n",
    "    return x[0]**2 + 100*x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad2(x):\n",
    "    return np.array([2*x[0], 200*x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1.2, 1.2, 100)\n",
    "y = np.linspace(-1.2, 1.2, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "levels = [0.1,1,2,4,9, 16, 25, 36, 49, 64, 81, 100]\n",
    "Z = x**2 + 100*Y**2\n",
    "c = plt.contour(X, Y, Z, levels)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd2(x, grad, alpha, max_iter=10):\n",
    "    xs = np.zeros((1 + max_iter, x.shape[0]))\n",
    "    xs[0,:] = x\n",
    "    for i in range(max_iter):\n",
    "        x = x - alpha * grad(x)\n",
    "        xs[i+1,:] = x\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd2_momentum(x, grad, alpha, beta=0.9, max_iter=10):\n",
    "    xs = np.zeros((1 + max_iter, x.shape[0]))\n",
    "    xs[0, :] = x\n",
    "    v = 0\n",
    "    for i in range(max_iter):\n",
    "        v = beta*v + (1-beta)*grad(x)\n",
    "        vc = v/(1+beta**(i+1))\n",
    "        x = x - alpha * vc\n",
    "        xs[i+1, :] = x\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with large step size\n",
    "\n",
    "We get severe oscillations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "x0 = np.array([-1,-1])\n",
    "xs = gd2(x0, grad2, alpha, max_iter=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1.2, 1.2, 100)\n",
    "y = np.linspace(-1.2, 1.2, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "levels = [0.1,1,2,4,9, 16, 25, 36, 49, 64, 81, 100]\n",
    "Z = x**2 + 100*Y**2\n",
    "c = plt.contour(X, Y, Z, levels)\n",
    "plt.plot(xs[:, 0], xs[:, 1], 'o-', c='red')\n",
    "plt.title('Vanilla gradient descent')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with momentum\n",
    "\n",
    "The damping effect is clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "x0 = np.array([-1,-1])\n",
    "xs = gd2_momentum(x0, grad2, alpha, beta=0.9, max_iter=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1.2, 1.2, 100)\n",
    "y = np.linspace(-1.2, 1.2, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "levels = [0.1,1,2,4,9, 16, 25, 36, 49, 64, 81, 100]\n",
    "Z = x**2 + 100*Y**2\n",
    "c = plt.contour(X, Y, Z, levels)\n",
    "plt.plot(xs[:, 0], xs[:, 1], 'o-', c='red')\n",
    "plt.title('Gradieent descent with momentum')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with RMSprop\n",
    "\n",
    "RMSprop scales the learning rate in each direction by the square root of the exponentially weighted sum of squared gradients. Near a saddle or any plateau, there are directions where the gradient is very small - RMSporp encourages larger steps in those directions, allowing faster escape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd2_rmsprop(x, grad, alpha, beta=0.9, eps=1e-8, max_iter=10):\n",
    "    xs = np.zeros((1 + max_iter, x.shape[0]))\n",
    "    xs[0, :] = x\n",
    "    v = 0\n",
    "    for i in range(max_iter):\n",
    "        v = beta*v + (1-beta)*grad(x)**2\n",
    "        x = x - alpha * grad(x) / (eps + np.sqrt(v))\n",
    "        xs[i+1, :] = x\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "x0 = np.array([-1,-1])\n",
    "xs = gd2_rmsprop(x0, grad2, alpha, beta=0.9, max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1.2, 1.2, 100)\n",
    "y = np.linspace(-1.2, 1.2, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "levels = [0.1,1,2,4,9, 16, 25, 36, 49, 64, 81, 100]\n",
    "Z = x**2 + 100*Y**2\n",
    "c = plt.contour(X, Y, Z, levels)\n",
    "plt.plot(xs[:, 0], xs[:, 1], 'o-', c='red')\n",
    "plt.title('Gradient descent with RMSprop')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAM\n",
    "\n",
    "ADAM (Adaptive Moment Estimation) combines the ideas of momentum, RMSprop and bias correction. It is probably the most popular gradient descent method in current deep learning practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd2_adam(x, grad, alpha, beta1=0.9, beta2=0.999, eps=1e-8, max_iter=10):\n",
    "    xs = np.zeros((1 + max_iter, x.shape[0]))\n",
    "    xs[0, :] = x\n",
    "    m = 0\n",
    "    v = 0\n",
    "    for i in range(max_iter):\n",
    "        m = beta1*m + (1-beta1)*grad(x)\n",
    "        v = beta2*v + (1-beta2)*grad(x)**2\n",
    "        mc = m/(1+beta1**(i+1))\n",
    "        vc = v/(1+beta2**(i+1))\n",
    "        x = x - alpha * m / (eps + np.sqrt(vc))\n",
    "        xs[i+1, :] = x\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "x0 = np.array([-1,-1])\n",
    "xs = gd2_adam(x0, grad2, alpha, beta1=0.9, beta2=0.9, max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1.2, 1.2, 100)\n",
    "y = np.linspace(-1.2, 1.2, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "levels = [0.1,1,2,4,9, 16, 25, 36, 49, 64, 81, 100]\n",
    "Z = x**2 + 100*Y**2\n",
    "c = plt.contour(X, Y, Z, levels)\n",
    "plt.plot(xs[:, 0], xs[:, 1], 'o-', c='red')\n",
    "plt.title('Gradient descent with RMSprop')\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
