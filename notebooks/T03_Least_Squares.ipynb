{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear least squares "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "import numpy as np\n",
    "import scipy.linalg as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal equations\n",
    "\n",
    "- Suppose $Ax = b$ is inconsistent. In this case, we can look instead for $\\widehat{x}$ which minimizes the distance between $Ax$ and $b$. In other words, we need to minimize $\\Vert Ax - b \\Vert^2$.\n",
    "- The minimum will occur when $\\langle Ax-b, Ax \\rangle = 0$ \n",
    "- Solving $(Ax)^T(Ax - b) = 0$ gives the normal equations $\\widehat{x} = (A^TA)^{-1}A^T b$\n",
    "- The corresponding vector in $C(A)$ is $A\\widehat{x} = A(A^TA)^{-1}A^T b$\n",
    "- Note that $P_A = A(A^TA)^{-1}A^T$ is the projection matrix for $C(A)$\n",
    "- This makes sense, since any solution to $Ax = b$ must be in $C(A)$, and the nearest such vector to $b$ must be the projection of $b$ onto $C(A)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "n = 5\n",
    "A = np.random.rand(m, n)\n",
    "b = np.random.rand(m, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using least squares function (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, res, rank, s, = la.lstsq(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A @ x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using normal equations (projection) - for illustration only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.inv(A.T @ A) @ A.T @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculus for normal equations\n",
    "\n",
    "In least squares problems, we usually have $m$ labeled observations $(x_i, y_i)$. We have a model that will predict $y_i$ given $x_i$ for some parameters $\\beta$, $f(x) = X\\beta$. We want to minimize the sum (or average) of squared residuals $r(x_i) = y_i - f(x_i)$. For example, the objective function is usually taken to be \n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\sum{r(x_i)^2}\n",
    "$$\n",
    "\n",
    "As a concrete example, suppose we want to fit a quadratic function to some observed data. We have\n",
    "\n",
    "$$\n",
    "f(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2\n",
    "$$\n",
    "\n",
    "We want to minimize the objective function\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2} \\sum_{i=1}^m (y_i - f(x_i))^2\n",
    "$$\n",
    "\n",
    "Taking derivatives with respect to $\\beta$, we get\n",
    "\n",
    "$$\n",
    "\\frac{dL}{d\\beta} = \n",
    "\\begin{bmatrix}\n",
    "\\sum_{i=1}^m f(x_i) - y_i \\\\\n",
    "\\sum_{i=1}^m x_i (f(x_i) - y_i) \\\\\n",
    "\\sum_{i=1}^m x_i^2 (f(x_i) - y_i)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Writing the above system as a matrix, we have $f(x) = X\\beta$, with\n",
    "\n",
    "$$ \n",
    "X = \\begin{bmatrix}\n",
    "1 & x_1 & x_1^2 \\\\\n",
    "1 & x_2 & x_2^2 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_m & x_m^2 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\beta = \\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\beta_2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We want to find the derivative of $\\Vert y - X\\beta \\Vert^2$, so\n",
    "\n",
    "$$\n",
    "\\Vert y - X\\beta \\Vert^2 \\\\\n",
    "= (y - X\\beta)^T(y - X\\beta) \\\\\n",
    "= (y^T - \\beta^TX^T)(y - X\\beta) \\\\\n",
    "= y^Ty - \\beta^TX^Ty -y^TX\\beta + \\beta^TX^TX\\beta\n",
    "$$\n",
    "\n",
    "Taking derivatives with respect to $\\beta^T$ (we do this because the gradient is traditionally a row vector, and we want it as a column vector here), we get (after multiplying by $1/2$ for the residue function)\n",
    "\n",
    "$$\n",
    "\\frac{dL}{d\\beta^T} =  X^TX\\beta - X^Ty\n",
    "$$\n",
    "\n",
    "For example, if we are doing gradient descent, the update equation is\n",
    "\n",
    "$$\n",
    "\\beta_{k+1} = \\beta_k + \\alpha (X^TX\\beta - X^Ty)\n",
    "$$\n",
    "\n",
    "Note that if we set the derivative to zero and solve, we get\n",
    "\n",
    "$$\n",
    "X^TX\\beta - X^Ty = 0\n",
    "$$\n",
    "\n",
    "and the normal equations\n",
    "\n",
    "$$\n",
    "\\beta = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "For large $X$, solving the normal equations can be more expensive than simpler gradient descent. Note that the Levenberg-Marquadt algorithm is often used to optimize least squares problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection matrices\n",
    "\n",
    "- Let $P$ be the projection matrix onto $C(A)$\n",
    "- Since it is a projection operator, $P^2$ = $P$ (check)\n",
    "- Check that $I-P$ is also idempotent (it turns out to also be a projection matrix)\n",
    "- Where does $I-P$ project to?\n",
    "- Show $C(I-P) = N(P)$\n",
    "- Show $\\rho(P) + \\nu(P) = n$\n",
    "- This implies $\\mathbb{R}^n = C(P) \\oplus C(I-P)$\n",
    "- Trivial example\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}, (I-P) = \\begin{bmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- Geometry of $P$ and $I-P$ in linear least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = A @ la.inv(A.T @ A) @ A.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, res, rank, s, = la.lstsq(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.eye(m) - P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(P @ b)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A @ x)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.norm(Q @ b)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "Note that $\\Vert Ax - b \\Vert^2$ can also be considered as a *cost* function of $x$ that we want to minimize. Hence, we can also use optimization methods such as gradient descent to solve this problem iteratively. Importantly, optimization techniques are generalizable to nonlinear cost functions as well, and some can be made to scale to massive problems.\n",
    "\n",
    "This will be the topic of the next set of lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
